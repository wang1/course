[
  {
    "file": "0.webm",
    "title": "00-AI概述",
    "desc": "本视频来自漫士沉思录，简要介绍了人工智能的发展历程与当前的主要技术。",
    "notes": [
      {"time": "00:00", "text": "作者简介"},
      {"time": "00:49", "text": "AI 是如何发展起来的"},
      {"time": "01:53", "text": "什么是智能"},
      {"time": "05:19", "text": "图灵测试及实现思路"},
      {"time": "10:45", "text": "机器学习要素"},
      {"time": "21:26", "text": "单层感知机的局限"},
      {"time": "24:40", "text": "突破-多层感知机"},
      {"time": "29:08", "text": "学习-梯度下降"},
      {"time": "46:05", "text": "提取模式-泛化"},
      {"time": "50:25", "text": "AI 万能吗"},
      {"time": "53:03", "text": "ChatGPT 原理"},
      {"time": "01:10:39", "text": "扩散模型-文生图/视频"},
      {"time": "01:25:32", "text": "AI会让我失业吗"}
    ]
  },
    {
    "file": "1.mp4",
    "title": "01-什么是神经网络",
    "desc": "什么是神经元？为何要分层？前向传播的数学表达是什么？ <a href='https://www.3blue1brown.com/lessons/neural-networks' target='_blank'>&#x1F449; 文字版 </a>",
    "notes": [
      {"time": "00:00", "text": "引例"},
      {"time": "01:07", "text": "预览"},
      {"time": "02:42", "text": "什么是神经元"},
      {"time": "03:35", "text": "层的引入"},
      {"time": "05:31", "text": "为何要分层"},
      {"time": "08:38", "text": "边缘检测"},
      {"time": "10:14", "text": "加权和、偏差、激活函数"},
      {"time": "11:35", "text": "总共有多少参数"},
      {"time": "12:30", "text": "学习：调整参数"},
      {"time": "13:26", "text": "符号和线性代数"},
      {"time": "15:17", "text": "回顾"},
      {"time": "16:27", "text": "结语"},
      {"time": "17:03", "text": "ReLU与Sigmod的比较"}
    ]
  },
  {
    "file": "2.mp4",
    "title": "02-神经网络如何学习：梯度下降",
    "desc": "本视频解释了梯度下降，这是神经网络训练中的核心概念。它使用成本/损失函数来说明神经网络如何学习识别手写数字。视频还探讨了网络的架构以及调整权重和偏差的过程。 <a href='https://www.3blue1brown.com/lessons/gradient-descent' target='_blank'>&#x1F449; 文字版-1, <a href='https://www.3blue1brown.com/lessons/neural-network-analysis' target='_blank'> 文字版-2</a></a>",
    "notes": [
      {"time": "00:00", "text": "介绍"},
      {"time": "00:30", "text": "回顾"},
      {"time": "01:49", "text": "使用训练数据"},
      {"time": "03:01", "text": "损失函数"},
      {"time": "06:55", "text": "梯度下降"},
      {"time": "11:18", "text": "梯度向量"},
      {"time": "12:19", "text": "梯度下降回顾"},
      {"time": "13:01", "text": "分析神经网络"},
      {"time": "14:01", "text": "真的在识别边和形状吗"},
      {"time": "16:37", "text": "更多学习资源"},
      {"time": "17:38", "text": "Lisha采访"},
      {"time": "19:58", "text": "结束语"}
    ]
  },
  {
    "file": "3.mp4",
    "title": "03-反向传播：直观理解（选学）",
    "desc": "当神经网络学习的时候到底发生了什么？本视频没有使用复杂的方程而是使用视觉示例，直观地解释了神经网络中的反向传播，展示了单个训练示例如何影响网络权重和偏差，然后调整这些值以最小化成本/损失的过程。 <a href='https://www.3blue1brown.com/lessons/backpropagation' target='_blank'>&#x1F449; 文字版</a><br><a href='./practice.html' target='_blank'>使用LLM构建图片识别应用</a>",
    "notes": [
      {"time": "00:00", "text": "介绍"},
      {"time": "00:23", "text": "回顾"},
      {"time": "03:07", "text": "直观的演练示例"},
      {"time": "09:33", "text": "随机梯度下降SGD"},
      {"time": "12:28", "text": "结束语"}
    ]
  },
    {
    "file": "4.mp4",
    "title": "04-反向传播：导数计算（选学）",
    "desc": "本视频探讨了神经网络中的反向传播中的导数计算。它使用一个简单的网络，直观地演示了链式法则在计算导数中的应用，随后将这一概念扩展到每层有多个神经元的网络中。 <a href='https://www.3blue1brown.com/lessons/backpropagation-calculus' target='_blank'>&#x1F449; 文字版 </a>",
    "notes": [
      {"time": "00:00", "text": "介绍"},
      {"time": "00:38", "text": "链式法则"},
      {"time": "03:56", "text": "导数计算"},
      {"time": "04:45", "text": "这些导数意味什么"},
      {"time": "05:39", "text": "权重和偏差的敏感度"},
      {"time": "06:42", "text": "具有额外神经元的层"},
      {"time": "09:13", "text": "总结"}
    ]
  },
    {
    "file": "5.mp4",
    "title": "05-LLM简介",
    "desc": "本视频使用引人入胜的动画来解释大型语言模型。它探讨了这些模型如何预测句子中的下一个单词，并通过例子来说明这个过程。视频还进一步深入探讨了训练的技术方面和transformers的作用。 <a href='https://www.3blue1brown.com/lessons/mini-llm' target='_blank'>&#x1F449; 文字版 </a>",
    "notes": [
    ]
  },
    {
    "file": "6.mp4",
    "title": "06-LLM背后的技术：Transformer",
    "desc": "这个视频从视觉上解释了大型语言模型（LLMs）总体的工作原理，重点介绍了Transformer。它逐步追踪数据在模型中的流动，展示了诸如词嵌入和softmax函数等关键概念。视频还探讨了LLMs如何预测序列中的下一个单词，逐步构建更长的文本生成。 <a href='https://www.3blue1brown.com/lessons/gpt' target='_blank'>&#x1F449; 文字版 </a><br><a href='https://gpt-tokenizer.dev/' target='_blank'> 分词示例 </a><a href='./pics/replaceByAI.jpg' target='_blank'> 矩阵乘法梗图 </a>",
    "notes": [
      {"time": "00:00", "text": "预测、重组、重复"},
      {"time": "03:03", "text": "Transformer窥探"},
      {"time": "06:36", "text": "章节布局"},
      {"time": "07:20", "text": "深度学习的先决条件"},
      {"time": "12:27", "text": "词嵌入"},
      {"time": "18:25", "text": "词嵌入之外"},
      {"time": "20:22", "text": "去除嵌入"},
      {"time": "22:22", "text": "Softmax with temperature"},
      {"time": "26:03", "text": "下一步"}
    ]
  },
    {
    "file": "7.mp4",
    "title": "07-语境整合器：Attention（选学）",
    "desc": "本视频逐步解释了在Transformer中的注意力机制（动态整合上下文信息，丰富了语义）。它通过实例说明来揭示自注意力、多头注意力和交叉注意力。了解注意力如何使模型能够对信息进行上下文化并更新嵌入。 <a href='https://www.3blue1brown.com/lessons/attention' target='_blank'>&#x1F449; 文字版 </a>",
    "notes": [
      {"time": "00:00", "text": "词嵌入回顾"},
      {"time": "01:39", "text": "引例"},
      {"time": "04:29", "text": "注意力模式"},
      {"time": "11:08", "text": "遮罩Masking"},
      {"time": "12:42", "text": "上下文大小"},
      {"time": "13:10", "text": "更新词嵌入"},
      {"time": "15:44", "text": "参数计算"},
      {"time": "18:21", "text": "注意力交叉机制"},
      {"time": "19:19", "text": "多头注意力"},
      {"time": "22:16", "text": "输出矩阵(可忽略)"},
      {"time": "23:19", "text": "多层的参数计算"},
      {"time": "24:54", "text": "结语"}
    ]
  },
    {
    "file": "8.mp4",
    "title": "08-知识存储器：MLP（选学）",
    "desc": "本视频探讨了大型语言模型（LLMs）如何存储知识（表明MLP内化与激活学到的知识）。它通过“迈克尔·乔丹打篮球”的例子，解析了Transformer架构中的多层感知器。教程使用动画来可视化其中涉及的复杂数学运算。 <a href='https://www.3blue1brown.com/lessons/mlp' target='_blank'>&#x1F449; 文字版 </a><br><a href='https://poloclub.github.io/transformer-explainer' target='_blank'> LLM工作流程可视化 </a>",
    "notes": [
      {"time": "00:00", "text": "LLM中事实的存放位置"},
      {"time": "02:15", "text": "快速回顾Transformer"},
      {"time": "04:39", "text": "示例的假设"},
      {"time": "06:07", "text": "多层感知机窥探"},
      {"time": "15:38", "text": "参数计算"},
      {"time": "17:04", "text": "叠加Superposition"},
      {"time": "21:37", "text": "下一步"}
    ]
  }
]
